# -*- coding: utf-8 -*-
"""torch_texts_dl.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C3qcZ5LXzNohklkCmXv4EJo5Mc3A2dt4
"""

### using the spacy for processing text and torch text pipelines and pandas for exploring and cleaning our data

# Step 2: Download the dataset using wget
!wget https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip

# Step 3: Unzip the file
!unzip trainingandtestdata.zip



import pandas as pd
# Step 4: Load the CSV into a DataFrame
# The dataset file is 'training.1600000.processed.noemoticon.csv'
tweetsDF = pd.read_csv("training.1600000.processed.noemoticon.csv",
                       header=None,
                       encoding='latin-1')  # Encoding ensures proper loading of special characters

# Step 5: Display the first few rows
tweetsDF.head()

tweetsDF[0].value_counts()

tweetsDF["sentiment_cat"] = tweetsDF[0].astype("category")
tweetsDF["sentiment"] = tweetsDF["sentiment_cat"].cat.codes
tweetsDF.head()

tweetsDF.to_csv("train-processed.csv", header=None, index=None)
tweetsDF.sample(10000).to_csv("train-processed-sample.csv", header=None, index=None)

!pip uninstall -y torchtext
!pip install torchtext==0.6.0

!python --version

import torch
torch.__version__

import torchtext

torchtext.__version__

from torchtext import data

LABEL = data.LabelField()
TWEET = data.Field(lower=True)

fields = [('score',None), ('id',None),('date',None),('query',None),
 ('name',None),
 ('tweet', TWEET),('category',None),('label',LABEL)]

twitterDataset = torchtext.data.TabularDataset(
 path="train-processed.csv",
 format="CSV",
 fields=fields,
 skip_header=False)

(train, test, valid) = twitterDataset.split(split_ratio=[0.8, 0.1, 0.1])

(len(train),len(test),len(valid))

vars(train.examples[7])

vocab_size = 20000
TWEET.build_vocab(train, max_size = vocab_size)

len(TWEET.vocab)

TWEET.vocab.freqs.most_common(10)

train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(
 (train, valid, test),
 batch_size = 32)

import torch.nn as nn

class Lstm(nn.Module):
  def __init__(self, hidden_size, embedding_dim, vocab_size):
    super(Lstm, self).__init__()
    self.embedding = nn.Embedding(vocab_size, embedding_dim)
    self.encoder = nn.LSTM(input_size=embedding_dim,
                           hidden_size=hidden_size, num_layers=1)
    self.predictor = nn.Linear(hidden_size, 2)

  def forward(self, seq):
    output, (hidden,_) = self.encoder(self.embedding(seq))
    preds = self.predictor(hidden.squeeze(0))
    return preds

model = Lstm(100,300,20002)

from torch import optim

optimizer = optim.Adam(model.parameters(), lr=2e-2)
criterion = nn.CrossEntropyLoss()

if torch.cuda.is_available():
  device = torch.device("cuda")
else:
  device = torch.device("cpu")

def train(epochs, model, optimizer, criterion, train_iterator, valid_iterator):
  for epoch in range(1, epochs + 1):
    training_loss = 0.0
    valid_loss = 0.0
    model.train()

    for batch_idx, batch in enumerate(train_iterator):
      optimizer.zero_grad()
      predict = model(batch.tweet)
      loss = criterion(predict, batch.label)
      loss.backward()
      optimizer.step()
      training_loss += loss.data.item() * batch.tweet.size(0)

    training_loss /= len(train_iterator)
    print('Epoch: {}, Training Loss: {:.2f}'.format(epoch, training_loss))

    model.eval()
    for batch_idx, batch in enumerate(valid_iterator):
      predict = model(batch.tweet)
      loss = criterion(predict, batch.label)
      valid_loss += loss.data.item() * batch.tweet.size(0)

    valid_loss /= len(valid_iterator)
    print('Epoch: {}, Validation Loss: {:.2f}'.format(epoch, valid_loss))

train(5, model, optimizer, criterion, train_iterator, valid_iterator)

##### back translation for text augmuntation

!pip install googletrans==4.0.0-rc1

from googletrans import Translator

# Initialize the translator
translator = Translator()

# Text to translate
sentences = ['The cat sat on the mat']

# Translate each sentence to French
translations_fr = [translator.translate(sentence, dest='fr').text for sentence in sentences]
print("French Translations:", translations_fr)

# Translate the French translations back to English
translations_en = [translator.translate(text, dest='en').text for text in translations_fr]
print("Back to English:", translations_en)

import random
from googletrans import Translator, LANGUAGES

# Initialize the translator
translator = Translator()

# Define the sentences to translate
sentences = ['The cat sat on the mat']

# Select a random language
available_langs = list(LANGUAGES.keys())
random_lang = random.choice(available_langs)

print(f"Translating to {LANGUAGES[random_lang]} ({random_lang})")

# Translate each sentence to the random language
translations = [translator.translate(sentence, dest=random_lang).text for sentence in sentences]
print("Translated Text:", translations)

# Translate back to English
translations_en = [translator.translate(text, src=random_lang, dest='en').text for text in translations]
print("Back to English:", translations_en)

